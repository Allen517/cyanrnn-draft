\relax 
\citation{kempe2003maximizing}
\citation{goyal2010learning}
\citation{gomez2013modeling}
\citation{WangAAAI15}
\citation{Manavoglu2003userbehaviormodels}
\citation{goldberg2014word2vec}
\citation{mikolov2010recurrent}
\citation{sundermeyer2012lstm}
\citation{DuKDD2016}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{fig:mot}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  An example of crossing dependency problem in sequence modeling. }}{1}}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {section}{\numberline {2}Model}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background}{2}}
\newlabel{eq:likelihood}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The architecture of CYAN-RNN. The figure presents the case when modeling the generation of the $(k+1)$-th propagation. The sequence at bottom is the observed propagations and the sequence at top is the predictive propagations. The blue rectangles refer to representations of hidden units. The yellow rectangle is a general form of attention function $s_k=\textit  {AttentionFunc}(t_{k-1}, h_k)$.}}{2}}
\newlabel{fig:cyrnn_frame}{{2}{2}}
\newlabel{eq:pooling_frame}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}CYAN-RNN}{2}}
\citation{DuKDD2016}
\citation{tu2016modeling}
\citation{tu2016modeling}
\newlabel{eq:cond_prob}{{3}{3}}
\newlabel{eq:target_embedding}{{4}{3}}
\newlabel{eq:alpha}{{5}{3}}
\newlabel{eq:score}{{6}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}CYAN-RNN with Coverage}{3}}
\newlabel{sec:coverage}{{2.3}{3}}
\newlabel{fig:att}{{3(a)}{3}}
\newlabel{sub@fig:att}{{(a)}{3}}
\newlabel{fig:cov}{{3(b)}{3}}
\newlabel{sub@fig:cov}{{(b)}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two kinds of implementation on attention layer. (a) The attention mechanism applied in CYAN-RNN; (b) The attention mechanism with coverage applied in CYAN-RNN (cov). Note that $\textbf  {h}_k=(h_1,\ldots  ,h_k)$ is matrix assembled by all historical propagation embeddings at step $k$, and $\textbf  {V}_k=(V_1,\ldots  ,V_k)$ is a coverage martix containing all coverage vectors at step $k$.}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}}
\newlabel{eq:cov}{{8}{3}}
\newlabel{eq:att_cov}{{9}{3}}
\citation{chung2014empirical}
\citation{chauvin1995backpropagation}
\citation{kingma2015method}
\citation{henaff2016orthogonal}
\citation{prechelt1998automatic}
\citation{goyal2010learning}
\citation{vere1988introduction}
\citation{hawkes1971spectra}
\citation{DuKDD2016}
\citation{LeskovecICML07}
\citation{LeskovecWWW08}
\citation{Erdos60}
\citation{voorhees1999trec}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Window Size}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Optimization}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{4}}
\newlabel{sec:exp}{{4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Baselines}{4}}
\newlabel{eq:hawke_intens_func}{{10}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Synthetic Data and Results Anlaysis}{4}}
\newlabel{fig:pred_usr_toy_cp_exp}{{4(a)}{5}}
\newlabel{sub@fig:pred_usr_toy_cp_exp}{{(a)}{5}}
\newlabel{fig:pred_usr_toy_cp_ray}{{4(b)}{5}}
\newlabel{sub@fig:pred_usr_toy_cp_ray}{{(b)}{5}}
\newlabel{fig:pred_usr_toy_rand_exp}{{4(c)}{5}}
\newlabel{sub@fig:pred_usr_toy_rand_exp}{{(c)}{5}}
\newlabel{fig:pred_usr_toy_rand_ray}{{4(d)}{5}}
\newlabel{sub@fig:pred_usr_toy_rand_ray}{{(d)}{5}}
\newlabel{fig:pred_tm_toy_rmse}{{4(e)}{5}}
\newlabel{sub@fig:pred_tm_toy_rmse}{{(e)}{5}}
\newlabel{fig:pred_usr_real}{{4(f)}{5}}
\newlabel{sub@fig:pred_usr_real}{{(f)}{5}}
\newlabel{fig:pred_tm_real_rmse}{{4(g)}{5}}
\newlabel{sub@fig:pred_tm_real_rmse}{{(g)}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Comparisons on baselines and our proposed models. (a)$\sim $(e) The predictions of next activated user and time on toy data produced from different networks and propagation models; (f) and (g) The predictions of next activated user and time on real data.}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CP, Exp}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {CP, Rayleign}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Random, Exp}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Random, Rayleign}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {RMSE on toy data}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Real data}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {RMSE on real data}}}{5}}
\newlabel{fig:cyan_deps}{{5(a)}{5}}
\newlabel{sub@fig:cyan_deps}{{(a)}{5}}
\newlabel{fig:cyan_cov_deps}{{5(b)}{5}}
\newlabel{sub@fig:cyan_cov_deps}{{(b)}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sample alignments on a fragement of cascade. The y-axis is the users who will be activated next sequentiallly from top to bottom. The x-axis is the activated order related to next activated user in the cascade. Each pixel shows the weight $\alpha _{k,i}$ related to $i$-th propagation embedding at each step $k$, in grayscale (0:black, 1:white). (a) the alignments inferred by CYAN-RNN; (b) the alignments inferred by CYAN-RNN(cov). }}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces AUC values of network inference.}}{6}}
\newlabel{t:auc_net_inf}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Real Data and Result Analysis}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{6}}
\bibstyle{named}
\bibdata{/home/yqwang/Documents/bibtex/social}
\bibcite{bahdanau2014neural}{\citeauthoryear {Bahdanau \bgroup \em  et al.\egroup }{2014}}
\bibcite{chauvin1995backpropagation}{\citeauthoryear {Chauvin and Rumelhart}{1995}}
\bibcite{chung2014empirical}{\citeauthoryear {Chung \bgroup \em  et al.\egroup }{2014}}
\bibcite{DuKDD2016}{\citeauthoryear {Du \bgroup \em  et al.\egroup }{2016}}
\bibcite{Erdos60}{\citeauthoryear {Erd\H {o}s and R{\'e}nyi}{1960}}
\bibcite{goldberg2014word2vec}{\citeauthoryear {Goldberg and Levy}{2014}}
\bibcite{gomez2013modeling}{\citeauthoryear {Gomez-Rodriguez \bgroup \em  et al.\egroup }{2013}}
\bibcite{goyal2010learning}{\citeauthoryear {Goyal \bgroup \em  et al.\egroup }{2010}}
\bibcite{hawkes1971spectra}{\citeauthoryear {Hawkes}{1971}}
\bibcite{henaff2016orthogonal}{\citeauthoryear {Henaff \bgroup \em  et al.\egroup }{2016}}
\bibcite{kempe2003maximizing}{\citeauthoryear {Kempe \bgroup \em  et al.\egroup }{2003}}
\bibcite{kingma2015method}{\citeauthoryear {Kingma and Adam}{2015}}
\bibcite{LeskovecICML07}{\citeauthoryear {Leskovec and Faloutsos}{2007}}
\bibcite{LeskovecWWW08}{\citeauthoryear {Leskovec \bgroup \em  et al.\egroup }{2008}}
\bibcite{Manavoglu2003userbehaviormodels}{\citeauthoryear {Manavoglu \bgroup \em  et al.\egroup }{2003}}
\bibcite{mikolov2010recurrent}{\citeauthoryear {Mikolov \bgroup \em  et al.\egroup }{2010}}
\bibcite{prechelt1998automatic}{\citeauthoryear {Prechelt}{1998}}
\bibcite{sundermeyer2012lstm}{\citeauthoryear {Sundermeyer \bgroup \em  et al.\egroup }{2012}}
\bibcite{tu2016modeling}{\citeauthoryear {Tu \bgroup \em  et al.\egroup }{2016}}
\bibcite{vere1988introduction}{\citeauthoryear {Vere-Jones}{1988}}
\bibcite{voorhees1999trec}{\citeauthoryear {Voorhees}{1999}}
\bibcite{WangAAAI15}{\citeauthoryear {Wang \bgroup \em  et al.\egroup }{2015}}
