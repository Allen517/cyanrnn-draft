\section{Optimization}

In this section, we introduce the learning process of our proposed models. 
The $k$-th propagation is transformed into vector as inputs including user
embedding and temporal features. The embedding matrix of
users is learned along with the training process. The temporal features
are formalized, inlcuding logarithm time interval
$\log(t_k-t_{k-1})$ and discretization of numerical attributes on year, month,
day, week, hour, mininute and second. We adopt
GRU~\cite{chung2014empirical} to encode the $k$-th inputs to propagation
embeddings $h_k$. Back-propagation through time
(BPTT)~\cite{chauvin1995backpropagation} is applied for parameter estimation.
The parameters are iteratively updated by Adam~\cite{kingma2015method}, an
efficient stochastic optimization algorithm with mini-batch techniques. 
For speeding up the convergence, we use
orthogonal initialization method~\cite{henaff2016orthogonal} in training
process. We also
employ early stopping method~\cite{prechelt1998automatic} to prevent
overfitting. The stopping criterion is achieved when the performance has no
more improvement in validation set. 
