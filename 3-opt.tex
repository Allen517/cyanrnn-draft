\section{Optimization}

In this section, we introduce the learning process of CYAN-RNN. 
The $k$-th propagation are transformed into vectors as inputs including user
embedding and temporal features. The user embedding matrix related to activated
users is learned along with the training process. The temporal features
related to activated time are formalized, inlcuding logarithm time interval
$\log(t_k-t_{k-1})$ and discretization of numerical attributes on year, month,
day, week, hour, mininute and second. We adopt
GRU~\cite{chung2014empirical} to encode the inputs of source sequence to event
embeddings $h$. We apply back-propagation through time
(BPTT)~\cite{chauvin1995backpropagation} for parameter estimation. The
parameters are iteratively updated by Adam~\cite{kingma2015method}, an
efficient stochastic optimization algorithm with mini-batch techniques. We also
employ early stopping method~\cite{prechelt1998automatic} to prevent
overfitting. The stopping criterion is achieved when the performance has no
more improvement in validation set. For speeding up the convergence, we use
orthogonal initialization method~\cite{henaff2016orthogonal} in training
process.
